[
["index.html", "Data Visualization Preface", " Data Visualization 2019 Preface "],
["project-workflow-and-style-guide.html", "1 Project Workflow and Style Guide 1.1 Directory Setup 1.2 Files 1.3 Code Guidelines 1.4 Output 1.5 Citing/Documenting Data", " 1 Project Workflow and Style Guide This chapter details the basic workflow that you should follow for data scicence projects in this course. These guidelines are designed to enable: The creation of clear code and reports Effective communication of ideas and results Reproducibility You may find these guidelines helpful for projects outside of the classroom as well! 1.1 Directory Setup Basic organization starts with your directory setup. A logical directory structure should have a home folder dedicated to the course and files dedicated to each assignment within your home folder or a subfolder. A great way to facilitate clear organization and sharing of data science projects in R is through the use of R project files (.Rproj). By using R project files, your file directories are relative to your project folder. To learn more about R project files, see Chapter 8 of R for Data Science. 1.2 Files For most assignments you will be asked to provide three different file types: An R script (.R) An Rmarkdown file (.Rmd) A compiled Rmarkdown report (.html) 1.2.1 R scripts R scripts should be provided to allow for quick reproducibility of your results. All code relevant to your analysis should be included and organized in a logical flow. Someone else should be able to take your R script and data and run it on their computer without making significant changes to it (provided it part of an R project). It should be well commented and organized according to the guidelines below. 1.2.2 Rmarkdown The Rmarkdown report provides a polished walkthrough of your code, as well as your answers to lab questions. Include your lab answers as plain text outside of embedded code (i.e., do not provide your answers as comments in code). It should be formatted using Rmarkdown syntax and organized using headers to separate lab sections and questions. Code should be broken up into chunks and included in the appropriate sections. If you will be writing mathematical expressions, you should invoke LaTeX math mode - surround your expressions with $ to process them inline, or $$ to center the output in a new line. For example: \\[\\int_{a}^{\\infty} dx\\,x^{m-2r} = - \\frac{a^{m-2r+1}}{m-2r+1}\\] If you are new to LaTeX, this website can help you construct equations easily. See the section on Output below for further advice on how to write your Rmarkdown reports. 1.3 Code Guidelines 1.3.1 Comments Comments should be used to organize and clarify code to make it as accessible as possible. They should not include commentary or answers to lab questions (see Rmarkdown section above), but should describe what your code does and how it works. To clarify code, comments should be used to explain intermediate steps in calculations, user-defined functions, functions from uncommon packages, and anything else that may be unclear from an initial read-through of your code. Comments can also be used to organize R scripts with headers and “lines” created with stretches of ### (not as necessary in Rmarkown since code chunks accomplish this). 1.3.2 Packages All libraries should be loaded at the beginning of an R script. Sometimes packages have functions with the same name, which can cause errors. In this case, the order in which the packages are loaded will impact your code. These conflicts can be avoided by specifying the package before calling the function or dataset of interest. For example, if both dplyr and MASS are loaded, you can specify that you wish to use select() from the dplyr packages like this: dplyr::select(). 1.3.3 Tidyverse The tidyverse is designed to facilitate fast and clear data science. The general rule in this course is to use tidyverse functions and principles when possible, and when it does not add significat complexity to your code. Tabular data should be read in as or converted to tibbles for most analyses, bearing in mind that some functions will require you to convert data into another format, such as lists or matrices. Use the pipe operator %&gt;%, read as “then”, to break up long chains of assignments and manipulations. 1.3.4 Naming Objects In this course, and in line with tidyverse principles, we will use snake case (e.g., stock_means, cool_function()). Make an effort to avoid vague or uninformative names such as x, y, data1, and var. Do not create objects with the same name as existing functions; for example, c() and lm.fit() are already functions in R, and should therefore not be used as names for other objects. To assign values to objects, use the arrow operator &lt;-. Although using = to assign values to objects is valid R code, we will limit its use to define function arguments. 1.3.5 Spacing and Indentation Use one space on each side of an operator (e.g., =, &lt;-, +) and one space after the comma in function arguments. For example: mpg_mean &lt;- mean(cars$mpg, na.rm = TRUE). Do not rely on RStudio or your text editor to wrap your code; instead break up long stretches of code with a new line at appropriate places, like after a comma in arguments. RStudio will automatically indent. You should indent when you are inside a function, inside a loop, when a stretch of code is being broken up, and after the first line when breaking up a stretch of code with %&gt;% or +. 1.4 Output For most assignments, the output of interest will be statistics, small tables, and graphics. Keep in mind that presentation is an important element of the Rmarkdown reports. Therefore, do not print long vectors or tables in your report. The use of tibbles should help mitigate the inclusion of long tables, as only the first few rows are printed. If you wish to include a large table or list of values with your assignment, write it out as a .csv file using write_csv() and upload it with your script files. Small tables may be printed out, or you can consider the use of kable() from the knitr package, as well as the kableExtra package. See below for guidelines on graphics. 1.4.1 Graphics Graphics should be created with ggplot() when possible. All axes should be appropriately labeled and legends included when appropriate. Exercise good judgement. Do not include several, full-size graphics of a similar nature without faceting or grouping together. If you must plot several scatterplots, consider the use of pairs() or ggpairs() but ensure that the output is not too small to read. 1.5 Citing/Documenting Data Proper data management, documentation, and citation is critical for good data science. Be sure to properly cite a dataset just as you would any other academic resource. If your resource is dynamic (changes with time), be sure to keep a copy (if permissions allow) and record when you obtained it. Avoid manually editing your dataset (e.g., in a text editor or Excel). These changes are very difficult to trace and can lead to irreproducible analyses. If it is necessary to edit your dataset, create a script to do so where you can detail the changes made and why you made them. Northwestern University Library has recommended these sources for how to cite your data: Digital Curation Center http://www.dcc.ac.uk/resources/how-guides/cite-datasets Northwestern http://libguides.northwestern.edu/datamanagement/citingdata IASSIST (APA, MLA, Chicago) https://www.icpsr.umich.edu/files/ICPSR/enewsletters/iassist.html 1.5.1 Sample Citation Studnitzer, Joshua, 2015, “Simplicity Versus WAR: Examining Salary Determinations in Major League Baseball’s Arbitration and Free Agent Markets”, https://doi.org/10.7910/DVN/28782, Harvard Dataverse, V3 "],
["stat-301-2-final-project.html", "2 STAT 301-2 Final Project 2.1 Milestones 2.2 Submission", " 2 STAT 301-2 Final Project Aside from the labs, you will complete a final project to showcase your foundational analytical abilities and model-building skills on a dataset of your choosing. You will locate a dataset, clean and join it as necessary, conduct an exploratory data analysis, build models to answer research questions (can be predictive or inferential), and compile a report on your results. 2.1 Milestones Submission of data memo (end of first 1/3 of course). You will prepare a data memo, including but not limited to the following: Proposed project timeline. When do you expect to have your dataset loaded into R? When do you expect to start your analysis? Simple overview of the dataset. What does the dataset document? How do you plan to collect it? (Is it a simple download, a webscrape, etc? Provide a formal citation.) How big is the dataset? What kinds of variables will you be dealing with? Any missingness? Do you need to join two or more sources of data together? Description of potential research questions. Are they predictive or inferential? Are these questions best answered by a classification or regression-based approach? What is the response variable? Which variables do you suspect will be useful in modeling the response? Any difficulties you may encounter along the way. Is the data collection mechanism complicated? Is there significant missingness in the data? The memo should conform to the style guide and should be completed in RMarkdown. Neatness, organization, and reproducibility of your memo will have a significant influence on your grade! Report rough draft (nearing the end of the quarter). Your report should be started well before finals week, when the final submission is due. The rough draft should be essentially complete by reading week, pending some graphics or final tabulated results. We make this stipulation for two reasons. First, you’ll deliver a presentation during reading week on your work, which depends on having the work mostly done. Second, if anything unexpected arises, you can get help from the instructors well before the due date! Essential components of a complete final report include, but are not limited to: Introduce your data and research question with a couple paragraphs An exploratory data analysis that is either motivated by or leads naturally to your research question(s), including illustrative tables and graphics Attempts to fit a couple different models and some notion of how each model performs, either using a validation set (okay) or cross-validation (preferred) – make sure you use appropriate performance measures! The performance of your best model on a performance set – as before, make sure it is an appropriate performance measure! Is the performance satisfactory? Debrief and next steps: what additional data resources would help improve the performance of your model? Which features of the model you selected make it the best (e.g. fits nonlinearity well)? Do any new research questions arise? A smooth, descriptive narrative that binds all of the above into a readable report Final report and executive summary presentation (end of quarter). The final report should include the components above. Your analysis scripts and project file will be submitted alongside the final report. In addition, you will produce an executive summary detailing the high points of your analysis – think of it as driving the same narrative as your report, really fast. The summary should be composed of graphics, tables, and bullet points organized in a compelling manner. You will present your executive summary in small groups. Anticipate 8-10 minutes of presentation, and another 5 minutes for questions. 2.2 Submission Your final project will be submitted on Canvas. You should include your report (.Rmd + built .html or .pdf) and executive summary (.ppt/x or .pdf), your analysis script (.R), the R project file (.Rproj), and your raw data files as one compressed file (.zip preferred, but other formats are acceptable, e.g. .tar.gz). The instructors should be able to download your compressed file, dump the contents into a directory, open the project file in RStudio, and run the script without errors. Neatness, organization, and reproducibility of your project will have a significant influence on your grade! "],
["lab-linear-regression.html", "3 Lab: Linear Regression 3.1 Libraries 3.2 Simple Linear Regression 3.3 Fitting Many Models 3.4 Qualitative Predictors 3.5 Modified Workflow", " 3 Lab: Linear Regression This is a modified version of the Lab: Linear Regression section of chapter 3 from Introduction to Statistical Learning with Application in R. This version uses tidyverse techniques and methods that will allow for scalability and a more efficient data analytic pipeline. 3.1 Libraries The library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR package, which includes the data sets associated with this book. library(tidyverse) library(modelr) library(janitor) library(skimr) library(broom) library(corrplot) library(ggfortify) If you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(&quot;ISLR&quot;). This installation only needs to be done the first time you use a package. However, the library() function must be called each time you wish to use a given package. 3.2 Simple Linear Regression We will begin our exploration of linear regression with simple linear regression. As alluded to in the name, this is the simplest form of a linear model which occurs when we only have one predictor variable (i.e., an equation for a line). The general model equation is provided below. Note that there are to parameters (\\(\\beta_0\\), the intercept; \\(\\beta_1\\) the slope) to estimate. \\[Y = \\beta_0 + \\beta_1X\\] Let’s practice fitting a simple linear model to a dataset. We will be using the Boston dataset from the MASS library, which records medv (median house value) for 506 suburbs of Boston. Eventually we will want to predict median home value (medv) using the 13 other predictors in the dataset such as rm (average number of rooms per house), age (percentage homes built prior to 1940), and lstat (percent of households with low socioeconomic status). As always we should examine the codebook for the dataset which can be accessed using ?MASS::Boston — you may need to install MASS first. While we could access the data directly from MASS we will instead load it from a text file separated by | (Boston.txt) which has been provided. We do this to continue practicing the implementation of a coding structure that allows us to both scale and easily modify our workflow. Additionally it keeps us thinking about how best to prepare/process our data. # clean_names() is not really needed - column names are already snake_lower_case boston_dat &lt;- read_delim(&quot;data/Boston.txt&quot;, delim = &quot;|&quot;) %&gt;% clean_names() ## Parsed with column specification: ## cols( ## crim = col_double(), ## zn = col_double(), ## indus = col_double(), ## chas = col_double(), ## nox = col_double(), ## rm = col_double(), ## age = col_double(), ## dis = col_double(), ## rad = col_double(), ## tax = col_double(), ## ptratio = col_double(), ## black = col_double(), ## lstat = col_double(), ## medv = col_double() ## ) Let’s get a quick overview of our data. boston_dat %&gt;% skim() %&gt;% # Used to improve output display kable() ## Skim summary statistics ## n obs: 506 ## n variables: 14 ## ## Variable type: numeric ## ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## ---------- --------- ---------- ----- -------- -------- -------- -------- -------- -------- ------- ---------- ## age 0 506 506 68.57 28.15 2.9 45.02 77.5 94.07 100 &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2587&gt; ## black 0 506 506 356.67 91.29 0.32 375.38 391.44 396.23 396.9 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt; ## chas 0 506 506 0.069 0.25 0 0 0 0 1 &lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## crim 0 506 506 3.61 8.6 0.0063 0.082 0.26 3.68 88.98 &lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## dis 0 506 506 3.8 2.11 1.13 2.1 3.21 5.19 12.13 &lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## indus 0 506 506 11.14 6.86 0.46 5.19 9.69 18.1 27.74 &lt;U+2583&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## lstat 0 506 506 12.65 7.14 1.73 6.95 11.36 16.96 37.97 &lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## medv 0 506 506 22.53 9.2 5 17.02 21.2 25 50 &lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## nox 0 506 506 0.55 0.12 0.38 0.45 0.54 0.62 0.87 &lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## ptratio 0 506 506 18.46 2.16 12.6 17.4 19.05 20.2 22 &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2583&gt; ## rad 0 506 506 9.55 8.71 1 4 5 24 24 &lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2585&gt; ## rm 0 506 506 6.28 0.7 3.56 5.89 6.21 6.62 8.78 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## tax 0 506 506 408.24 168.54 187 279 330 666 711 &lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2586&gt; ## zn 0 506 506 11.36 23.32 0 0 0 12.5 100 &lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; We see there is no missing data. That medv ranges from $5,000 to $50,000, which seems a little strange. Can you explain why we shouldn’t be surprised by this range of values (see codebook)? Let’s take a quick look at a correlogram to help us pick a single predictor for our simple linear model. boston_dat %&gt;% cor() %&gt;% corrplot() We see that lstat has a sizable correlation with medv. So let’s use lstat as our predictor variable. We see from the table above that lstat ranges from 1.7% to 38%. Meaning we have a suburb were about 1 in every 50 households is considered to be low socioeconomic status and one suburb where about 20 in every 50 households are considered low. The model we want to fit is \\[\\mbox{mdev} = \\beta_0 + \\beta_1\\mbox{lstat}\\] We will be using the lm() function to fit our simple linear regression model. It is good practice to investigate the R documentation for an unfamiliar function (or even a familiar function) in order to understand what inputs are required, default settings/inputs, and what output will be produced (use ?lm). The basic syntax is lm(formula = y ∼ x, data = dataset) or lm(y ∼ x, dataset), where y ~ x defines the formula for the model we wish to fit (y is the response, x is the predictor), and dataset contains these two variables. Read ~ as predicted by so we read y ~ x as y predicted by x. Some important notes (1) the data is the second argument — important for piping — and (2) R automatically includes a constant term (i.e., the intercept). Let’s fit our simple linear model. # Three ways to fit &amp; store the model (2 with piping, 1 without piping) lm_fit &lt;- boston_dat %&gt;% lm(formula = medv ~ lstat) lm_fit &lt;- boston_dat %&gt;% lm(medv ~ lstat, data = .) lm_fit &lt;- lm(medv ~ lstat , data = boston_dat) # What is stored lm_fit ## ## Call: ## lm(formula = medv ~ lstat, data = boston_dat) ## ## Coefficients: ## (Intercept) lstat ## 34.55 -0.95 lm_fit returns some basic information about the model is output, but more detailed information, we could use summary(lm_fit). This gives us p-values and standard errors for the coefficients, as well as the R2 statistic and F-statistic for the model. lm_fit %&gt;% summary() ## ## Call: ## lm(formula = medv ~ lstat, data = boston_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.168 -3.990 -1.318 2.034 24.500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.55384 0.56263 61.41 &lt;2e-16 *** ## lstat -0.95005 0.03873 -24.53 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.216 on 504 degrees of freedom ## Multiple R-squared: 0.5441, Adjusted R-squared: 0.5432 ## F-statistic: 601.6 on 1 and 504 DF, p-value: &lt; 2.2e-16 Unfortunately this is not very tidy and incorporating summary() into a analytic pipeline can be difficult. Thankfully we have the broom package which provides three very helpful function (see Introduction to broom): tidy(): constructs a data frame that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions. augment(): add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments. glance(): construct a concise one-row summary of the model. This typically contains values such as R2, adjusted R2, and residual standard error that are computed once for the entire model. lm_fit %&gt;% tidy() %&gt;% clean_names() ## # A tibble: 2 x 5 ## term estimate std_error statistic p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.6 0.563 61.4 3.74e-236 ## 2 lstat -0.950 0.0387 -24.5 5.08e- 88 lm_fit %&gt;% augment() %&gt;% clean_names() ## # A tibble: 506 x 9 ## medv lstat fitted se_fit resid hat sigma cooksd std_resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 24 4.98 29.8 0.406 -5.82 0.00426 6.22 0.00189 -0.939 ## 2 21.6 9.14 25.9 0.308 -4.27 0.00246 6.22 0.000582 -0.688 ## 3 34.7 4.03 30.7 0.433 3.97 0.00486 6.22 0.00100 0.641 ## 4 33.4 2.94 31.8 0.467 1.64 0.00564 6.22 0.000198 0.264 ## 5 36.2 5.33 29.5 0.396 6.71 0.00406 6.21 0.00238 1.08 ## 6 28.7 5.21 29.6 0.399 -0.904 0.00413 6.22 0.0000440 -0.146 ## 7 22.9 12.4 22.7 0.276 0.155 0.00198 6.22 0.000000620 0.0250 ## 8 27.1 19.2 16.4 0.374 10.7 0.00362 6.20 0.00544 1.73 ## 9 16.5 29.9 6.12 0.724 10.4 0.0136 6.20 0.0194 1.68 ## 10 18.9 17.1 18.3 0.326 0.592 0.00274 6.22 0.0000125 0.0954 ## # ... with 496 more rows lm_fit %&gt;% glance() %&gt;% clean_names() ## # A tibble: 1 x 11 ## r_squared adj_r_squared sigma statistic p_value df log_lik aic ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.544 0.543 6.22 602. 5.08e-88 2 -1641. 3289. ## # ... with 3 more variables: bic &lt;dbl&gt;, deviance &lt;dbl&gt;, df_residual &lt;int&gt; These three function allow us to extract a vast amount of useful information from a linear model and store it as a tibble. This will allow us to smoothly integrate this information into an analytic pipeline/workflow. In order to obtain a confidence interval for our model’s parameters/coefficients, we can use confint_tidy instead of confint(). lm_fit %&gt;% confint_tidy() %&gt;% clean_names() ## # A tibble: 2 x 2 ## conf_low conf_high ## &lt;dbl&gt; &lt;dbl&gt; ## 1 33.4 35.7 ## 2 -1.03 -0.874 By default it constructs 95% confidence intervals for each parameter in our model. Unfortunately it does not include an information detailing which interval belongs to which parameter. Be default it goes in order of inclusion into the model and the intercept is always first. This is intentional because in most cases this information is combined with information from tidy() which does include such information. lm_fit %&gt;% tidy() %&gt;% bind_cols(lm_fit %&gt;% confint_tidy()) %&gt;% clean_names() ## # A tibble: 2 x 7 ## term estimate std_error statistic p_value conf_low conf_high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.6 0.563 61.4 3.74e-236 33.4 35.7 ## 2 lstat -0.950 0.0387 -24.5 5.08e- 88 -1.03 -0.874 The predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat — predict() does not have a tidy analog. new_data &lt;- tibble(lstat = c(5, 10, 15)) new_data %&gt;% predict(lm_fit, newdata = ., interval = &quot;confidence&quot;) %&gt;% as_tibble() ## # A tibble: 3 x 3 ## fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29.8 29.0 30.6 ## 2 25.1 24.5 25.6 ## 3 20.3 19.7 20.9 new_data %&gt;% predict(lm_fit, newdata = ., interval = &quot;prediction&quot;) %&gt;% as_tibble() ## # A tibble: 3 x 3 ## fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29.8 17.6 42.0 ## 2 25.1 12.8 37.3 ## 3 20.3 8.08 32.5 For instance, the 95% confidence interval associated with a lstat value of 10 is (24.47, 25.63), and the 95% prediction interval is (12.828, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when lstat equals 10), but the prediction interval is much wider. Why is that the case? The confidence interval is a range of plausible values for the expected/average response value. The prediction interval provides a plausible range of values for the response. 3.2.1 Plots for Assessing Linear Models Graphical techniques are essential tools for assessing and communicating models. We will attempt to use ggplot2 techniques whenever possible, but there are built-in diagnostic plots in base R which are useful and will get the job done. Our preference for ggplot2 is due to it being a core tidyverse package — makes building workflows/pipelines easier — and we can quickly and efficiently build highly customizable graphics. We will need to have a dataset that contains both the original data and the additional variables from a fitted model. That is exactly what augment() does. Check out ?augment.lm for details. boston_augmented &lt;- boston_dat %&gt;% augment(lm_fit, data = .) %&gt;% clean_names() Before fitting a linear model it would be wise to check that the relationship between the response and predictor(s) is linear. While it might seem obvious to do, it is a step that is unfortunately ignored more often than you would think. In fact, we skipped this step above and just went directly to fitting a linear model. We present two ways to construct this plot. One is more general and will be useful when fitting more advanced or non-standard models. # General plot boston_augmented %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_line(aes(y = fitted), color = &quot;blue&quot;, size = 1) # geom_smooth() depends on defined method boston_augmented %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Added loess smooth for comparison geom_smooth(se = FALSE, color = &quot;red&quot;, linetype = &quot;dashed&quot;) Clearly the relationship is not linear. Maybe a polynomial fit would be better, say a quadratic which we will explore later. Next we will move on to a series of model diagnostic plots. This (tidyverse reference page)[https://ggplot2.tidyverse.org/reference/fortify.lm.html] provides an example of how to take the base R diagnostic plots and construct ggplot2 analogs — note that they use fortify() instead of augment(). boston_augmented %&gt;% ggplot(aes(x = fitted, y = resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_smooth(color = &quot;red&quot;, se = FALSE) boston_augmented %&gt;% ggplot(aes(x = fitted, y = std_resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_smooth(color = &quot;red&quot;, se = FALSE) boston_augmented %&gt;% ggplot() + stat_qq(aes(sample = std_resid)) + geom_abline() boston_augmented %&gt;% ggplot(aes(x = fitted, y = sqrt(abs(std_resid)))) + geom_point() + geom_smooth(color = &quot;red&quot;, se = FALSE) boston_augmented %&gt;% arrange(hat) %&gt;% ggplot(aes(seq_along(cooksd), cooksd)) + geom_col() boston_augmented %&gt;% ggplot(aes(hat, std_resid)) + geom_vline(size = 2, colour = &quot;white&quot;, xintercept = 0) + geom_hline(size = 2, colour = &quot;white&quot;, yintercept = 0) + geom_point() + geom_smooth(color = &quot;red&quot;, se = FALSE) boston_augmented %&gt;% ggplot(aes(hat, std_resid)) + geom_vline(size = 2, colour = &quot;white&quot;, xintercept = 0) + geom_hline(size = 2, colour = &quot;white&quot;, yintercept = 0) + geom_point(aes(size = cooksd)) + geom_smooth(color = &quot;red&quot;, se = FALSE) It is often useful to investigate or identify observations that are high leverage and or are influential. The hat values (leverage statistics) are used to identify high leverage observations — larger values indicate higher leverage. Cook’s distance (cooksd) are used to identify influential observations — larger values indicate higher influence. Now that we know which measures to use we can simply use arrange() and filter() to extract the observations of interest. Using filter() requires a cut off value (i.e. keep all observations larger than a value) which is a little problematic. We are only looking for the observations with the largest values, top_n() from dplyr to the rescue — see ?top_n for details. # 5 observations with highest leverage boston_augmented %&gt;% top_n(5, hat) %&gt;% arrange(desc(hat)) %&gt;% select(hat, medv, lstat, everything()) ## # A tibble: 5 x 21 ## hat medv lstat crim zn indus chas nox rm age dis rad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0269 13.8 38.0 18.5 0 18.1 0 0.668 4.14 100 1.14 24 ## 2 0.0250 7 37.0 45.7 0 18.1 0 0.693 4.52 100 1.66 24 ## 3 0.0210 13.8 34.8 11.1 0 18.1 0 0.668 4.91 100 1.17 24 ## 4 0.0204 14.4 34.4 1.63 0 21.9 0 0.624 5.02 100 1.44 4 ## 5 0.0203 17.9 34.4 18.8 0 18.1 0 0.597 4.63 100 1.55 24 ## # ... with 9 more variables: tax &lt;dbl&gt;, ptratio &lt;dbl&gt;, black &lt;dbl&gt;, ## # fitted &lt;dbl&gt;, se_fit &lt;dbl&gt;, resid &lt;dbl&gt;, sigma &lt;dbl&gt;, cooksd &lt;dbl&gt;, ## # std_resid &lt;dbl&gt; # 5 observations with highest cook&#39;s d boston_augmented %&gt;% top_n(5, cooksd) %&gt;% arrange(desc(cooksd)) %&gt;% select(cooksd, medv, lstat, everything()) ## # A tibble: 5 x 21 ## cooksd medv lstat crim zn indus chas nox rm age dis rad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0862 13.8 38.0 18.5 0 18.1 0 0.668 4.14 100 1.14 24 ## 2 0.0700 17.9 34.4 18.8 0 18.1 0 0.597 4.63 100 1.55 24 ## 3 0.0515 23.7 29.6 0.290 0 10.6 0 0.489 5.41 9.8 3.59 4 ## 4 0.0432 14.4 34.4 1.63 0 21.9 0 0.624 5.02 100 1.44 4 ## 5 0.0427 13.8 34.8 11.1 0 18.1 0 0.668 4.91 100 1.17 24 ## # ... with 9 more variables: tax &lt;dbl&gt;, ptratio &lt;dbl&gt;, black &lt;dbl&gt;, ## # fitted &lt;dbl&gt;, se_fit &lt;dbl&gt;, resid &lt;dbl&gt;, hat &lt;dbl&gt;, sigma &lt;dbl&gt;, ## # std_resid &lt;dbl&gt; Unfortunately the dataset does not supply the name/unique identifier for the Boston suburbs it contains. A unique identifier becomes useful when comparing lists like those above. We could have added a unique identifier using row_number() at many different points to assist with this. Could have done this when creating boston_augmented. Consider going back and doing this within your code in order to help you identify if there are both suburbs on both the high leverage and high influential lists above. Sidebar Base R is fairly adept at creating quick diagnostic plots for linear models, which is useful when working on a one-off project or for quick exploration. plot() will automatically produce four diagnostic plots for a lm() object. In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() function, which tells R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a 2×2 grid of panels. par(mfrow = c(2,2)) plot(lm_fit) There is a ggplot version of the diagnostic plots produced using plot(lm_fit). It requires the packages ggfortify and has been unstable in the past. lm_fit %&gt;% autoplot() 3.3 Fitting Many Models At the core of statistical/machine learning is the idea of fitting many models or slight variations of a model type for comparison. Conducting this process in an effective, efficient, and organized manner is extremely important. While the previous section provided an introduction on how to fit a simple linear model and how to examine it, this section will focus on how to incorporate these processes into a workflow that allows for the fitting and assessment of many candidate models. The map functions and nest()/unnest() from purrr will be essential. Our desire to fit many models naturally leads to multiple linear regression — multiple predictor. The syntax to do this in R is intuitive, for the most part. We just add the the desired variable to the formula definition. For instance, suppose we wanted to fit a linear model that uses a suburb’s age (percentage homes built prior to 1940) and lstat (percent of households with low socioeconomic status) to predict its medv (median home value). We would use medv ~ lstat + age for our R formula. Note that you should read the + as and. Now read the R formula as medv is predicted by lstat and age. Note that + is not a mathematical operator in this instance. The following is a list of seven candidate models we would like to fit. These models were selected to demonstrate important features of the R formula syntax. Simple linear regression using lstat medv ~ lstat Polynomial fit (quadratic in lstat) medv ~ lstat + I(lstat^2) or medv ~ poly(medv, 2) If you need to do a calculation involving a variable such as squaring or re-centering it, then you will need to wrap it in I(). Alternatively you could create the variable and add to your dataset. Use only lstat and age as predictors medv ~ lstat + age Allow for interaction between lstat and age medv ~ lstat + age + lstat:age or medv ~ lstat*age Note the using ensures that the main effects of the two variables are included. Using var_one:var_two only includes the interaction term which can be useful. Full model or kitchen sink model (all available variables as predictors) medv ~ ., Notice that we do not have to list all variables. Everything except for age medv ~ . - age We should - read as do not inclue or remove. All pair-wise interactions (including main effects) medv ~ .*. We begin by creating a tibble/dataset/database with a list-column variable named data and then immediately proceed to fit our models. The list-column structure is necessary for the mapping functions. We will want to do this in one pipeline, but it is instructive to take a peak at the initial step which can be accomplished using either nest() or tibble(). # Option 1 boston_dat %&gt;% nest() ## # A tibble: 1 x 1 ## data ## &lt;list&gt; ## 1 &lt;tibble [506 x 14]&gt; # Option 2 tibble(data = list(boston_dat)) ## # A tibble: 1 x 1 ## data ## &lt;list&gt; ## 1 &lt;tibble [506 x 14]&gt; The list-column structure may seem strange here since we only have one dataset, but imagine if we had the same Boston dataset updated annually. Then we could place each year’s data within the data list-column. This would be particularly useful because we could seamlessly fit each of the seven models to each year of data — this might not be clear now, but it will be as you become familiar with the mapping functions. We store the data and the corresponding models in boston_models. We made the choice to use model names that are uninformative (mod_01, …, mod_07) because it would be difficult to come up with a naming scheme that would be both useful and not overly cumbersome. boston_models &lt;- boston_dat %&gt;% nest() %&gt;% mutate(mod_01 = map(data, lm, formula = medv ~ lstat), mod_02 = map(data, lm, formula = medv ~ poly(lstat, 2)), mod_03 = map(data, lm, formula = medv ~ lstat + age), mod_04 = map(data, lm, formula = medv ~ lstat*age), mod_05 = map(data, lm, formula = medv ~ .), mod_06 = map(data, lm, formula = medv ~ . - age), mod_07 = map(data, lm, formula = medv ~ .*.)) boston_models ## # A tibble: 1 x 8 ## data mod_01 mod_02 mod_03 mod_04 mod_05 mod_06 mod_07 ## &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;tibble [506 x~ &lt;S3: lm&gt; &lt;S3: lm&gt; &lt;S3: l~ &lt;S3: l~ &lt;S3: l~ &lt;S3: l~ &lt;S3: l~ It should be clear that boston_models is not in a tidy format. The models are spread over several columns so we will need to gather them up. boston_models &lt;- boston_models %&gt;% gather(key = model_name, value = model_fit, -data) boston_models ## # A tibble: 7 x 3 ## data model_name model_fit ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; ## 1 &lt;tibble [506 x 14]&gt; mod_01 &lt;S3: lm&gt; ## 2 &lt;tibble [506 x 14]&gt; mod_02 &lt;S3: lm&gt; ## 3 &lt;tibble [506 x 14]&gt; mod_03 &lt;S3: lm&gt; ## 4 &lt;tibble [506 x 14]&gt; mod_04 &lt;S3: lm&gt; ## 5 &lt;tibble [506 x 14]&gt; mod_05 &lt;S3: lm&gt; ## 6 &lt;tibble [506 x 14]&gt; mod_06 &lt;S3: lm&gt; ## 7 &lt;tibble [506 x 14]&gt; mod_07 &lt;S3: lm&gt; Now we have a tidy database containing our fitted models and its corresponding data. We could save boston_models — save_rds() — for later usage. This is especially useful when model fitting is time consuming. 3.3.1 Assessing Many Models Let’s assess how each model fit the data before digging into the particulars of each model. The glance() functions returns many model assessment measures and can seamlessly applied to each model fit using map(). The trick is accessing the the information once it is added to the boston_models and recognizing it is added as list-column. The unnest() function is how we unpack the information contained in a list-column — see ?unnest() for details. # Assessing models with AIC boston_models %&gt;% mutate(mod_glance = map(model_fit, glance)) %&gt;% # .drop = TRUE: drops all other list-columns (get rid of cluter) unnest(mod_glance, .drop = TRUE) %&gt;% arrange(AIC) %&gt;% select(model_name, AIC, everything()) ## # A tibble: 7 x 12 ## model_name AIC r.squared adj.r.squared sigma statistic p.value df ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 mod_07 2581. 0.921 0.904 2.85 53.2 6.49e-181 92 ## 2 mod_06 3026. 0.741 0.734 4.74 117. 6.08e-136 13 ## 3 mod_05 3028. 0.741 0.734 4.75 108. 6.72e-135 14 ## 4 mod_02 3171. 0.641 0.639 5.52 449. 1.56e-112 3 ## 5 mod_04 3280. 0.556 0.553 6.15 209. 4.86e- 88 4 ## 6 mod_03 3283. 0.551 0.549 6.17 309. 2.98e- 88 3 ## 7 mod_01 3289. 0.544 0.543 6.22 602. 5.08e- 88 2 ## # ... with 4 more variables: logLik &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt; We can use a visualization to inspect the models on several assessment measures. We have included a few other model assessment measures from the modelr package — see ?mae for details on each. This also provides an opportunity to demonstrate the use of a map2 function. Note that the _dbl on these function is made necessary by the .drop = TRUE argument in unnest(). boston_models %&gt;% mutate(mod_glance = map(model_fit, glance), mae = map2_dbl(model_fit, data, mae), rmse = map2_dbl(model_fit, data, rmse), mape = map2_dbl(model_fit, data, mape)) %&gt;% unnest(mod_glance, .drop = TRUE) %&gt;% select(model_name, r.squared, adj.r.squared, AIC, BIC, deviance, sigma, rmse, mae, mape) %&gt;% gather(key = measure, value = value, -model_name) %&gt;% ggplot(aes(value, model_name)) + geom_point() + facet_wrap(. ~ measure, scales = &quot;free_x&quot;) We can quickly compare models using any of these assessment measures. The story is pretty much the same across the measures. Not surprisingly, the more flexible models (mod_05, mod_06, &amp; mod_07) do a much better job of fitting the data than the less flexible models. We also see that mod_02 is a vast improvement over mod_01. Why might that be? Should we be using this to determine which model might preform best on Boston suburb from 2018? No, we are using the same data to assess the model that we used to train/build it. We can use these assessment measures to determine which models fit this particular dataset the best and to develop some insight into the type of model we should consider using for future dataset. It will be useful to store the model related information from glance(), tidy(), confidnt_tidy(), and augment() within our model database. This is seamlessly achieved by using the map() function. We also used a map2() function to ensure the augment output was consistent across models and bind the output from tidy() and confint_tidy() together. boston_models &lt;- boston_models %&gt;% mutate(mod_glance = map(model_fit, glance), mod_tidy = map(model_fit, tidy), add_tidy = map(model_fit, confint_tidy), mod_tidy = map2(mod_tidy, add_tidy, bind_cols), mod_augment = map2(model_fit, data, augment)) %&gt;% select(-add_tidy) Now we have a tidy database containing our fitted models, its corresponding data, and assorted information concerning these models. Again this would be useful to save boston_models — save_rds() — for later usage. Let’s build a graphic to compare the estimated coefficients for a few predictor variables across the models. One way to do this is to plot the 95% confidence intervals per model for the variables of interest. We can achieve this by extracting the tidy information from boston_models, keep only the terms we would like to examine, and build the plot. boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(term %in% c(&quot;lstat&quot;, &quot;age&quot;)) %&gt;% ggplot(aes(model_name, estimate)) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + facet_wrap(. ~ term, scales = &quot;free_x&quot;) + coord_flip() Notice that mod_02, the polynomial fit, does not appear in these plots. This is actually a good thing because we want terms from each model to actually be comparable. The polynomial fit, mod_02, does estimate a coefficient for poly(lstat, 2)1 (the linear term), but it is not compatible to the other model’s estimates because poly() uses an orthogonalization fitting method — see poly() for details. We could address this by setting raw = TRUE in our pipeline’s poly() call, but we will leave that for you to attempt. Still it would be nice to see how the polynomial fit compares to the simple linear model we began with. Let’s plot the fitted models (mod_01 &amp; mod_02) and we will see that the polynomial model appears to fit the data better which is not surprising given the model assessment measures we have above. boston_models %&gt;% filter(model_name %in% c(&quot;mod_01&quot;, &quot;mod_02&quot;)) %&gt;% unnest(mod_augment) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_line(aes(y = .fitted, color = model_name), size = 1) 3.3.2 Examing One or Fewer Models The structure of boston_models allows for a workflow/pipeline that is extremely useful for exploring, assessing, and comparing many models. We avoid having many unnecessary intermediate objects to keep track of and we can quickly adjust our pipeline with any corrections or additions. What if we want to focus on one of the models contained in boston_models? One-off investigations or explorations can be useful as a check on your coding, for gaining insight into the data, or developing ideas for more models to fit. It would be nice to be able to quickly extract the desired information which in most instances can be done using some combination of filter(), select(), and unnest(), or R’s accessor syntax (e.g. $, [[]], []). Luckily purrr provides pluck(). pluck() is particularly useful for extracting elements from a list-column that are not in a tibble format with the same dimensions. Such as the model_fit list-column in boston_models. Suppose we want to quickly extract the fit for mod_02 and examine it. # Quick diagnostic plots boston_models %&gt;% pluck(&quot;model_fit&quot;, 1) %&gt;% # use plot() if autoplot() doesn&#39;t work autoplot() Unlike the diagnostic plots for mod_01 there is no discernible pattern in the residuals. We could also use analysis of variance (ANOVA) to examine this model and statistically compare it to mod_01. Note that mod_01 is a linear submodel mod_02. This is because the only difference between these models is the quadratic term in mod_02 (lstat2). If we set the coefficient on the quadratic term to 0 then mod_02 would reduce to mod_01 which is why mod_01 is a submodel of mod_02. We can use the anova() function to perform a hypothesis test comparing the two models (provided one is a submodel). The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model (not the submodel) is superior. To do this without pluck() can be a pain. # # Without pluck() # boston_models %&gt;% # filter(model_name %in% c(&quot;mod_01&quot;, &quot;mod_02&quot;)) %&gt;% # select(model_name, model_fit) %&gt;% # spread(key = model_name, value = model_fit) %&gt;% # transmute(test = map2(mod_01, mod_02, anova)) %&gt;% # unnest() anova(boston_models %&gt;% pluck(&quot;model_fit&quot;, 1), boston_models %&gt;% pluck(&quot;model_fit&quot;, 2)) ## Analysis of Variance Table ## ## Model 1: medv ~ lstat ## Model 2: medv ~ poly(lstat, 2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 504 19472 ## 2 503 15347 1 4125.1 135.2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. Maybe we want to examine the variance inflation factors for the full model, mod_05. The vif() function, part of the car package, can be used to compute variance inflation factors. We’ve added a few more steps to the pipeline to provide a useful presentation of the output. You may need to install the car package. boston_models %&gt;% pluck(&quot;model_fit&quot;, 5) %&gt;% car::vif() %&gt;% # Quick way to turn a named vector to a tibble enframe() %&gt;% arrange(desc(value)) ## # A tibble: 13 x 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 tax 9.01 ## 2 rad 7.48 ## 3 nox 4.39 ## 4 indus 3.99 ## 5 dis 3.96 ## 6 age 3.10 ## 7 lstat 2.94 ## 8 zn 2.30 ## 9 rm 1.93 ## 10 ptratio 1.80 ## 11 crim 1.79 ## 12 black 1.35 ## 13 chas 1.07 3.4 Qualitative Predictors We will attempt to predict car seat sales for 400 locations using a number of predictors — both quantitative and qualitative. The data is contained in the provided Carseats.csv file. It is also part of the ISLR package so it would be wise to inspect its codebook — ?ISLR::Carseats. Remember that instead of loading the data from ISLR we read it in from the provided file in order to continue practicing a coding structure that allows us to both scale and easily modify our workflow and it will keep us thinking about what steps are necessary to prepare/process our data. We provide two options for reading in the data. The first is useful when we want to re-type all character variables to factors and don’t have a preference for the order of the levels, which can be helpful for datasets with a large number of variables. In the second option we manually type the factor variables and decide on the ordering. We could use a hybrid of the options to quickly re-type variables and then follow it by re-leveling the factors we want re-leveled — use fct_relevel(). # Option 1 carseats_dat &lt;- read_csv(&quot;data/Carseats.csv&quot;) %&gt;% clean_names() %&gt;% mutate_if(is.character, as.factor) ## Parsed with column specification: ## cols( ## Sales = col_double(), ## CompPrice = col_double(), ## Income = col_double(), ## Advertising = col_double(), ## Population = col_double(), ## Price = col_double(), ## ShelveLoc = col_character(), ## Age = col_double(), ## Education = col_double(), ## Urban = col_character(), ## US = col_character() ## ) # Option 2 carseats_dat &lt;- read_csv(&quot;data/Carseats.csv&quot;) %&gt;% clean_names() %&gt;% mutate(shelve_loc = factor(shelve_loc, levels = c(&quot;Bad&quot;, &quot;Medium&quot;, &quot;Good&quot;)), urban = factor(urban, levels = c(&quot;No&quot;, &quot;Yes&quot;)), us = factor(urban, levels = c(&quot;No&quot;, &quot;Yes&quot;))) ## Parsed with column specification: ## cols( ## Sales = col_double(), ## CompPrice = col_double(), ## Income = col_double(), ## Advertising = col_double(), ## Population = col_double(), ## Price = col_double(), ## ShelveLoc = col_character(), ## Age = col_double(), ## Education = col_double(), ## Urban = col_character(), ## US = col_character() ## ) # Hybrid carseats_dat &lt;- read_csv(&quot;data/Carseats.csv&quot;) %&gt;% clean_names() %&gt;% mutate_if(is.character, as.factor) %&gt;% mutate(shelve_loc = fct_relevel(shelve_loc, &quot;Bad&quot;, &quot;Medium&quot;)) ## Parsed with column specification: ## cols( ## Sales = col_double(), ## CompPrice = col_double(), ## Income = col_double(), ## Advertising = col_double(), ## Population = col_double(), ## Price = col_double(), ## ShelveLoc = col_character(), ## Age = col_double(), ## Education = col_double(), ## Urban = col_character(), ## US = col_character() ## ) Let’s take a quick look at the data. carseats_dat %&gt;% skim() %&gt;% kable() ## Skim summary statistics ## n obs: 400 ## n variables: 11 ## ## Variable type: factor ## ## variable missing complete n n_unique top_counts ordered ## ------------ --------- ---------- ----- ---------- ----------------------------------- --------- ## shelve_loc 0 400 400 3 Med: 219, Bad: 96, Goo: 85, NA: 0 FALSE ## urban 0 400 400 2 Yes: 282, No: 118, NA: 0 FALSE ## us 0 400 400 2 Yes: 258, No: 142, NA: 0 FALSE ## ## Variable type: numeric ## ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## ------------- --------- ---------- ----- -------- -------- ---- ------- ------ ------- ------- ---------- ## advertising 0 400 400 6.63 6.65 0 0 5 12 29 &lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## age 0 400 400 53.32 16.2 25 39.75 54.5 66 80 &lt;U+2586&gt;&lt;U+2586&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2586&gt; ## comp_price 0 400 400 124.97 15.33 77 115 125 135 175 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## education 0 400 400 13.9 2.62 10 12 14 16 18 &lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2583&gt; ## income 0 400 400 68.66 27.99 21 42.75 69 91 120 &lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2586&gt;&lt;U+2585&gt; ## population 0 400 400 264.84 147.38 10 139 272 398.5 509 &lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2586&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt; ## price 0 400 400 115.8 23.68 24 100 117 131 191 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt; ## sales 0 400 400 7.5 2.82 0 5.39 7.49 9.32 16.27 &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt; The Carseats data includes qualitative predictors such as shelve_loc, an indicator of the quality of the shelving location — that is, the space within a store in which the car seat is displayed — at each location. The predictor shelv_loc takes on three possible values, Bad, Medium, and Good. Given a qualitative variable such as shelve_loc, R generates dummy variables automatically. Below we fit two simple linear regressions using shelve_loc where the only difference being that one includes an intercept term (mod_01) and the other does not (mod_02). We also fit a couple of multiple regression models. # Organize fitted models carseats_models &lt;- carseats_dat %&gt;% nest() %&gt;% mutate(mod_01 = map(data, lm, formula = sales ~ shelve_loc), mod_02 = map(data, lm, formula = sales ~ shelve_loc - 1), mod_03 = map(data, lm, formula = sales ~ .), mod_04 = map(data, lm, formula = sales ~ . + income:advertising + price:age)) %&gt;% gather(key = model_name, value = model_fit, -data) # Model fit information carseats_models &lt;- carseats_models %&gt;% mutate(mod_glance = map(model_fit, glance), mod_tidy = map(model_fit, tidy), add_tidy = map(model_fit, confint_tidy), mod_tidy = map2(mod_tidy, add_tidy, bind_cols), mod_augment = map2(model_fit, data, augment)) %&gt;% select(-add_tidy) Let’s begin by looking over the estimates produced by the first two models. Since there is no intercept in mod_02 the estimated coefficients are equal to the mean sales for each of our three shelve location categories. We see that the estimated intercept for mod_01 is the same as the estimate for shelve_locBad in the mod_02. Since Bad is the reference group in mod_01 the intercept for the model is equal to the mean sales for that group. The other coefficients in mod_01 provide the difference between the indicated category and the model’s reference group. Therefore, if we add the estimate for Good to the intercept in mod_01 we will get the mean sales for the Good shelve location category. Which is directly provided in mod_02. carseats_models %&gt;% filter(model_name %in% c(&quot;mod_01&quot;, &quot;mod_02&quot;)) %&gt;% unnest(mod_tidy) %&gt;% kable() model_name term estimate std.error statistic p.value conf.low conf.high mod_01 (Intercept) 5.522917 0.2387665 23.131038 0 5.053512 5.992321 mod_01 shelve_locMedium 1.783659 0.2863562 6.228811 0 1.220695 2.346623 mod_01 shelve_locGood 4.691083 0.3484201 13.463871 0 4.006104 5.376062 mod_02 shelve_locBad 5.522917 0.2387665 23.131038 0 5.053512 5.992321 mod_02 shelve_locMedium 7.306575 0.1580836 46.219681 0 6.995790 7.617361 mod_02 shelve_locGood 10.214000 0.2537462 40.252822 0 9.715146 10.712854 Is there really a difference between mod_01 and mod_02? No, there is no difference other than how the information is encoded and extracted from the model. We just need to do a little adding or subtracting to move from one coefficient/parameter estimate to the other. What is important to realize here is that even though the terms share a name (or common symbol) they do not always estimate the same quantity. Let’s move on and check if mod_04 fits the data better than mod_03. The only difference between these models is a few interactions terms. anova(carseats_models %&gt;% pluck(&quot;model_fit&quot;, 3), carseats_models %&gt;% pluck(&quot;model_fit&quot;, 4)) ## Analysis of Variance Table ## ## Model 1: sales ~ comp_price + income + advertising + population + price + ## shelve_loc + age + education + urban + us ## Model 2: sales ~ comp_price + income + advertising + population + price + ## shelve_loc + age + education + urban + us + income:advertising + ## price:age ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 388 402.83 ## 2 386 394.23 2 8.6045 4.2125 0.0155 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes, mod_04 does provide significantly better fit to the data. Let’s take a quick look at this model. carseats_models %&gt;% pluck(&quot;mod_tidy&quot;, 4) ## # A tibble: 14 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.58 1.01 6.52 2.22e- 10 4.59e+0 8.56 ## 2 comp_price 0.0929 0.00412 22.6 1.64e- 72 8.48e-2 0.101 ## 3 income 0.0109 0.00260 4.18 3.57e- 5 5.77e-3 0.0160 ## 4 advertising 0.0702 0.0226 3.11 2.03e- 3 2.58e-2 0.115 ## 5 population 0.000159 0.000368 0.433 6.65e- 1 -5.64e-4 0.000883 ## 6 price -0.101 0.00744 -13.5 1.74e- 34 -1.15e-1 -0.0862 ## 7 shelve_locMe~ 1.95 0.126 15.5 1.34e- 42 1.71e+0 2.20 ## 8 shelve_locGo~ 4.85 0.153 31.7 1.38e-109 4.55e+0 5.15 ## 9 age -0.0579 0.0160 -3.63 3.18e- 4 -8.93e-2 -0.0266 ## 10 education -0.0209 0.0196 -1.06 2.88e- 1 -5.94e-2 0.0177 ## 11 urbanYes 0.140 0.112 1.25 2.13e- 1 -8.08e-2 0.361 ## 12 usYes -0.158 0.149 -1.06 2.91e- 1 -4.50e-1 0.135 ## 13 income:adver~ 0.000751 0.000278 2.70 7.29e- 3 2.04e-4 0.00130 ## 14 price:age 0.000107 0.000133 0.801 4.24e- 1 -1.55e-4 0.000369 3.5 Modified Workflow The general work flow is the same, but maybe we are provided with a list of formulas or more likely it is more efficient for us to create a list of formulas. For instance, suppose we want to fit all possible simple linear regression for predicting medv in the boston_dat dataset. Creating this list of formulas is much easier than having to code each individual formula. The workflow also has the benefit of being able to seamlessly incorporate any additional variables that might be added to the Boston dataset. # Setup formulas predictor_var &lt;- boston_dat %&gt;% names() %&gt;% setdiff(&quot;medv&quot;) fmla &lt;- paste(&quot;medv ~&quot;, predictor_var) # Fit and store the models boston_models &lt;- tibble(data = list(boston_dat), model_name = c(predictor_var, &quot;full&quot;) , fmla = c(fmla, &quot;medv ~ .&quot;)) %&gt;% mutate(model_fit = map2(fmla, data, lm)) # Model fit summaries/information boston_models &lt;- boston_models %&gt;% mutate(mod_glance = map(model_fit, glance), mod_tidy = map(model_fit, tidy), add_tidy = map(model_fit, confint_tidy), mod_tidy = map2(mod_tidy, add_tidy, bind_cols), mod_augment = map2(model_fit, data, augment)) %&gt;% select(-add_tidy) # Scatterplot to compare SLR to Full estimates boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(model_type = if_else(model_name != &quot;full&quot;, &quot;slr&quot;, &quot;full&quot;)) %&gt;% select(model_type, term, estimate) %&gt;% spread(model_type, estimate) %&gt;% ggplot(aes(full, slr)) + geom_point() + geom_abline(color = &quot;blue&quot;, linetype = &quot;dashed&quot;) # Alternative to scatterplot boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(model_type = if_else(model_name != &quot;full&quot;, &quot;slr&quot;, &quot;full&quot;)) %&gt;% ggplot(aes(model_type, estimate)) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + geom_hline(yintercept = 0, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + facet_wrap(. ~ term, scales = &quot;free_x&quot;) + coord_flip() 3.5.1 Exercise 15 (Section 3.7 - pg 126) Let’s use this modified workflow to work through Exercise 15 from Section 3.7 of Introduction to Statistical Learning. Again it is important not to just copy and paste a workflow. You need to think about the process and what might be motivating the data structure we are building. This can only be achieved by reading through the question in its entirety and sketching our a plan of action that will allow us to answer the question. It is also useful to think beyond the questions being asked directly and anticipate indirect questions of explorations that may arise (one reason for using a flexible pipeline). Also, realize the resulting code that you see below is a cleaned up version of the process. You don’t see the iterative process by which we encounter issues and accordingly adjust the pipeline. Putting in the time to justify every line of code will help you understand pipeline development and analysis process. # Setup formulas for simple linear regressions = predictor_var &lt;- boston_dat %&gt;% names() %&gt;% setdiff(&quot;crim&quot;) fmla &lt;- paste(&quot;crim ~&quot;, predictor_var) # adding full model predictor_var &lt;- c(predictor_var, &quot;all_vars&quot;) fmla &lt;- c(fmla, &quot;crim ~ .&quot;) # Fit and store the models boston_models &lt;- tibble(data = list(boston_dat), predictor_var, fmla) %&gt;% mutate(model_fit = map2(fmla, data, lm), # add column for model type model_type = if_else(predictor_var == &quot;all_vars&quot;, &quot;full&quot;, &quot;slr&quot;)) # Model fit summaries/information boston_models &lt;- boston_models %&gt;% mutate(mod_glance = map(model_fit, glance), mod_tidy = map(model_fit, tidy), add_tidy = map(model_fit, confint_tidy), mod_tidy = map2(mod_tidy, add_tidy, bind_cols), mod_augment = map2(model_fit, data, augment)) %&gt;% select(-add_tidy) # Identify SLR models with significant slope/linear parameter (0.05) boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(model_type != &quot;full&quot;, term != &quot;(Intercept)&quot;) %&gt;% select(term, estimate, p.value) %&gt;% arrange(p.value) %&gt;% filter(p.value &lt; 0.05) %&gt;% kable() term estimate p.value rad 0.6179109 0.0e+00 tax 0.0297423 0.0e+00 lstat 0.5488048 0.0e+00 nox 31.2485312 0.0e+00 indus 0.5097763 0.0e+00 medv -0.3631599 0.0e+00 black -0.0362796 0.0e+00 dis -1.5509017 0.0e+00 age 0.1077862 0.0e+00 ptratio 1.1519828 0.0e+00 rm -2.6840512 6.0e-07 zn -0.0739350 5.5e-06 # Plot investigating linear rel. with crim boston_dat %&gt;% select(-chas) %&gt;% gather(key = predictor, value = value, -crim) %&gt;% ggplot(aes(x = value, y = crim)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + coord_cartesian(ylim = c(0, 25)) + facet_wrap(. ~ predictor, scales = &quot;free_x&quot;) # Ivestigating full model boston_models %&gt;% filter(model_type == &quot;full&quot;) %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% select(-predictor_var, -fmla, -model_type) %&gt;% kable() term estimate std.error statistic p.value conf.low conf.high (Intercept) 17.0332275 7.2349030 2.3543132 0.0189491 2.8181092 31.2483459 zn 0.0448552 0.0187341 2.3943122 0.0170249 0.0080466 0.0816639 indus -0.0638548 0.0834072 -0.7655789 0.4442940 -0.2277331 0.1000235 chas -0.7491336 1.1801468 -0.6347800 0.5258670 -3.0678829 1.5696156 nox -10.3135349 5.2755363 -1.9549737 0.0511520 -20.6788947 0.0518249 rm 0.4301305 0.6128303 0.7018754 0.4830888 -0.7739569 1.6342179 age 0.0014516 0.0179251 0.0809837 0.9354878 -0.0337676 0.0366709 dis -0.9871757 0.2818173 -3.5028930 0.0005022 -1.5408895 -0.4334619 rad 0.5882086 0.0880493 6.6804480 0.0000000 0.4152096 0.7612076 tax -0.0037800 0.0051556 -0.7331884 0.4637927 -0.0139097 0.0063497 ptratio -0.2710806 0.1864505 -1.4539010 0.1466113 -0.6374180 0.0952569 black -0.0075375 0.0036733 -2.0519589 0.0407023 -0.0147548 -0.0003202 lstat 0.1262114 0.0757248 1.6667104 0.0962084 -0.0225726 0.2749953 medv -0.1988868 0.0605160 -3.2865169 0.0010868 -0.3177885 -0.0799852 # Identify significant slope/linear parameters in full model (0.05) boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(model_type == &quot;full&quot;, term != &quot;(Intercept)&quot;) %&gt;% select(term, estimate, p.value) %&gt;% arrange(p.value) %&gt;% filter(p.value &lt; 0.05) %&gt;% kable() term estimate p.value rad 0.5882086 0.0000000 dis -0.9871757 0.0005022 medv -0.1988868 0.0010868 zn 0.0448552 0.0170249 black -0.0075375 0.0407023 # Scatterplot to compare SLR to Full estimates boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% select(model_type, term, estimate) %&gt;% spread(model_type, estimate) %&gt;% ggplot(aes(full, slr)) + geom_point() + geom_abline(color = &quot;blue&quot;, linetype = &quot;dashed&quot;) # Alternative to scatterplot boston_models %&gt;% unnest(mod_tidy, .drop = TRUE) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(model_type, estimate)) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) + geom_hline(yintercept = 0, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + facet_wrap(. ~ term, scales = &quot;free_x&quot;) + coord_flip() # Setup formulas for cubic models # --- REMOVE chas because you cannot fit a cubic to a binary var predictor_var &lt;- boston_dat %&gt;% names() %&gt;% setdiff(c(&quot;crim&quot;, &quot;chas&quot;)) fmla &lt;- paste0(&quot;crim ~ poly(&quot;, predictor_var, &quot;, 3)&quot;) # Fit and store the cubic models cubic_models &lt;- tibble(data = list(boston_dat), predictor_var, fmla) %&gt;% mutate(cubic_fit = map2(fmla, data, lm)) # ANOVA test to determine if cubic/nonlinear significantly fits the data # better than a simple linear regression (0.05) boston_models %&gt;% filter(model_type != &quot;full&quot;, predictor_var != &quot;chas&quot;) %&gt;% select(predictor_var, model_fit) %&gt;% left_join(cubic_models, by =&quot;predictor_var&quot;) %&gt;% mutate(anova_test = map2(model_fit, cubic_fit, anova)) %&gt;% unnest(anova_test, .drop = TRUE) %&gt;% drop_na() %&gt;% rename(term = predictor_var, p_value = `Pr(&gt;F)`) %&gt;% select(term, p_value) %&gt;% filter(p_value &lt; 0.05) %&gt;% arrange(p_value) %&gt;% kable() term p_value medv 0.0000000 dis 0.0000000 nox 0.0000000 indus 0.0000000 age 0.0000004 tax 0.0000114 ptratio 0.0002542 rm 0.0052294 zn 0.0085120 rad 0.0260783 lstat 0.0369832 "],
["lab-logistic-regression-lda-qda-and-knn.html", "4 Lab: Logistic Regression, LDA, QDA, and KNN 4.1 Data Setup 4.2 Logistic Regression 4.3 Linear Discriminant Analysis 4.4 Quadratic Discriminant Analysis 4.5 K-Nearest Neighbors", " 4 Lab: Logistic Regression, LDA, QDA, and KNN This is a modified version of the Lab: Logistic Regression, LDA, QDA, and KNN section of chapter 4 from Introduction to Statistical Learning with Application in R. This version uses tidyverse techniques and methods that will allow for scalability and a more efficient data analytic pipeline. We will need the packages listed below. Order matters! We strategically load tidyverse after MASS so that the package dplyr from the tidyverse is loaded second. We do this because both packages have a select() function, and R will attempt to use the most recently loaded version of a function. This is called masking and you’ll get these warnings when you load packages with such conflicts. We can use dplyr::select() or MASS::select() to force R to use the function version from their respective package. This is cumbersome, so by loading dplyr second we are able to avoid having to use dplyr:select(). # Load packages library(MASS) library(tidyverse) library(modelr) library(janitor) library(skimr) library(broom) library(corrplot) library(class) 4.1 Data Setup We have been discussing the concept and importance of training and testing data over the last few weeks. We will start to use it practice now by using what is sometimes called a hold-out dataset, which is really just a test dataset. Before we implement this process let’s introduce the data we will be using for this lab. We will be using the Smarket dataset from the ISLR library. Take a moment and inspect the codebook — ?ISLR::Smarket. This data set consists of percentage returns for the S&amp;P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). In order to continue practicing the implementation of a coding structure that allows us to both scale and easily modify our workflow we will load this data from the provided Smaket.csv file. Notice that we re-type all character variables as factors. smarket_dat &lt;- read_csv(&quot;data/Smarket.csv&quot;) %&gt;% clean_names() %&gt;% mutate_if(is_character, factor) We would typically proceed onto examining the data with numerical summaries and visualizations, but since we plan on creating a hold-out/test dataset we should do that first. We do not want aspects of the testing data to influence our modeling training/building process. There is a balancing act here because we first need to adequately process the data in order to ensure that both the training and testing datasets have the same data structure (e.g, variable types). Or we need to ensure that any data cleaning/processing pipeline we build for the training data can also be applied to the training data. smarket_dat is fairly simple so we can proceed directly to splitting our data into testing and training. Since this is time dependent data we will use data pre-2005 (2001-2004) for training and the 2005 data for testing. We will store these in a tibble data_db &lt;- tibble(train = list(smarket_dat %&gt;% filter(year &lt; 2005)), test = list(smarket_dat %&gt;% setdiff(train))) Now let’s examine the training potion of our data. data_db %&gt;% unnest(train) %&gt;% skim() %&gt;% kable() ## Skim summary statistics ## n obs: 998 ## n variables: 9 ## ## Variable type: factor ## ## variable missing complete n n_unique top_counts ordered ## ----------- --------- ---------- ----- ---------- -------------------------- --------- ## direction 0 998 998 2 Up: 507, Dow: 491, NA: 0 FALSE ## ## Variable type: numeric ## ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## ---------- --------- ---------- ----- --------- ------ ------- ------- ------- ------ ------ ---------- ## lag1 0 998 998 0.00096 1.23 -4.92 -0.71 0.021 0.66 5.73 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## lag2 0 998 998 0.00076 1.23 -4.92 -0.71 0.021 0.66 5.73 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## lag3 0 998 998 -0.0019 1.23 -4.92 -0.71 0.021 0.66 5.73 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## lag4 0 998 998 -0.0036 1.23 -4.92 -0.71 0.015 0.66 5.73 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## lag5 0 998 998 0.0018 1.24 -4.92 -0.71 0.021 0.66 5.73 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## today 0 998 998 0.00045 1.23 -4.92 -0.71 0.015 0.66 5.73 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## volume 0 998 998 1.37 0.27 0.36 1.21 1.37 1.51 2.78 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## year 0 998 998 2002.52 1.11 2001 2002 2003 2004 2004 &lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2587&gt; # Proportions of Up/Down days data_db %&gt;% unnest(train) %&gt;% count(direction) %&gt;% mutate(prop = n/sum(n)) ## # A tibble: 2 x 3 ## direction n prop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Down 491 0.492 ## 2 Up 507 0.508 # Correlations data_db %&gt;% unnest(train) %&gt;% select(-direction) %&gt;% cor() %&gt;% corrplot() # Time trend in volume data_db %&gt;% unnest(train) %&gt;% ggplot(aes(x = seq_along(volume), y = volume)) + geom_line(alpha = 0.3) + geom_smooth(se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # Checking time trend for other variables data_db %&gt;% unnest(train) %&gt;% gather(key = variable, value = value, -year, -direction) %&gt;% ggplot(aes(x = seq_along(value), y = value)) + geom_line(alpha = 0.3) + geom_smooth(se = FALSE) + theme(axis.text = element_blank(), axis.ticks = element_blank()) + facet_wrap(. ~ variable, scales = &quot;free&quot;) FALSE `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 4.2 Logistic Regression We will be using fit a few logistic regressions to attempt to predict whether the stock market will go down or up. Our set of possible predictors are the five lag measurements (lag1, …, lag5) and volume of volume of shares (in billions) traded on the previous day. We will train the models on the training dataset and then use the testing data set to validate and compare the models. We will begin by only fitting a logistic model that uses all available predictors, but set up our workflow to allow us to easily include alternatively specified logistic models. Let’s fit the model. glm_fits &lt;- data_db %&gt;% mutate(mod_01 = map(train, glm, formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + volume, family = binomial)) Just like with linear models we can use tidy(), confint_tidy(), glance(), augment() and predict() to inspect the model. While we could store this information in glm_fits we will hold off on this for now because we first need to understand what we will be storing and why we want it stored. Secondly it would throw a wrench into our general workflow for fitting several models (look back on how we fit several linear models). Let’s inspect the parameter estimates. glm_fits %&gt;% pluck(&quot;mod_01&quot;, 1) %&gt;% tidy() ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.191 0.334 0.573 0.567 ## 2 lag1 -0.0542 0.0518 -1.05 0.295 ## 3 lag2 -0.0458 0.0518 -0.884 0.377 ## 4 lag3 0.00720 0.0516 0.139 0.889 ## 5 lag4 0.00644 0.0517 0.125 0.901 ## 6 lag5 -0.00422 0.0511 -0.0826 0.934 ## 7 volume -0.116 0.240 -0.485 0.628 Nothing, not even the intercept, is significant in this model. Indicating that there is no statistical evidence of an association between any of these predictors and direction. While none of the associations were significant, we are probably more interested in how well the model predicted direction. We can extract the predicted values using augment(), the .fitted values, or predict(). However augment() includes additional measures for model diagnostics (e.g. .hat, .cooksd, etc.) which can be useful. We will opt to use predict() for now since we want to focus on assessing the predictive capabilities of our model. We can and should circle back to model diagnostics for a more comprehensive understanding of our fitted model. It is extremely important to understand what predict() returns for a logistic regression. Recall that we ultimately want a predicted direction (Up or Down). First note that without a newdata argument predict() returns a model’s predicted values for the data it was fitted/trained on. Next, let’s apply and actually inspect the values that predict() returns glm_fits %&gt;% pluck(&quot;mod_01&quot;, 1) %&gt;% predict() %&gt;% skim() ## ## Skim summary statistics ## ## -- Variable type:numeric --------------------------------------------------------------------------- ## variable missing complete n mean sd p0 p25 p50 p75 p100 ## . 0 998 998 0.032 0.093 -0.4 -0.021 0.036 0.088 0.4 ## hist ## &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt; We see predict() returns a numeric value and some are values are negative and some are positive. Since we use logistic regression to predict the probability of a specified category (either Up or Down in this case), we should be concerned with the negative values (probabilities cannot be negative). We need to realize that predict() is providing our predicted values on an alternative scale — log-odds for binomial link. This is easy to remedy by setting type = &quot;response&quot;, but it is easy to forget so take care with this — it also must be specified when using augment(). Now let’s inspect the predicted probabilities for the training data. glm_fits %&gt;% pluck(&quot;mod_01&quot;, 1) %&gt;% predict(type = &quot;response&quot;) %&gt;% skim() ## ## Skim summary statistics ## ## -- Variable type:numeric --------------------------------------------------------------------------- ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## . 0 998 998 0.51 0.023 0.4 0.49 0.51 0.52 0.6 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt; We now have predicted probabilities ranging from about 0.4 to 0.6. What are these probabilities for though? We know they are for a direction, but are they the probabilities for Up or Down? This has to do with how the direction factor is encoded because predict(type = &quot;response&quot;) returns the probability for the category that is designated to be 1 or in mathematical notation \\(Pr(Y_i=1|X_i)\\). Let’s check how direction was encoded when we loaded, processed, and assigned the data to smarket_dat. smarket_dat %&gt;% # pull(): extract a column from a tibble - replacement for $ pull(direction) %&gt;% contrasts() ## Up ## Down 0 ## Up 1 Since Up is assigned 1 we now know that the predicted probabilities are for Up. This is important because we can now change the predicted probabilities to predicted directions of the stock market. For observations with a predicted probability greater than 0.5 then we will assign it to Up, otherwise they will be assigned Down. We will use demo_tib to demonstrate how to calculate and inspect predictions. This is to avoid altering glm_fits which will house several fitted logistic models and their associated information. demo_tib &lt;- glm_fits %&gt;% mutate(train_prob = map(mod_01, predict, type = &quot;response&quot;), train_direction = map(train_prob, ~ if_else(.x &gt; 0.5, &quot;Up&quot;, &quot;Down&quot;))) # Predictions for train dataset demo_tib %&gt;% unnest(train, train_direction) %&gt;% count(train_direction) %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 2 x 3 ## train_direction n prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Down 331 0.332 ## 2 Up 667 0.668 # Confusion matrix for train dataset demo_tib %&gt;% unnest(train, train_direction) %&gt;% count(direction, train_direction) %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 4 x 4 ## direction train_direction n prop ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Down Down 175 0.175 ## 2 Down Up 316 0.317 ## 3 Up Down 156 0.156 ## 4 Up Up 351 0.352 # Model assessment (accuracy/error) for train dataset demo_tib %&gt;% unnest(train, train_direction) %&gt;% mutate(correct = if_else(train_direction == direction, 1, 0)) %&gt;% summarise(train_accuracy = mean(correct), train_error = 1 - train_accuracy) ## # A tibble: 1 x 2 ## train_accuracy train_error ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.527 0.473 # Model predictions for test dataset demo_tib &lt;- demo_tib %&gt;% mutate(test_prob = map2(mod_01, test, predict, type = &quot;response&quot;), test_direction = map(test_prob, ~ if_else(.x &gt; 0.5, &quot;Up&quot;, &quot;Down&quot;))) # Model assessment (accuracy/error) for test dataset demo_tib %&gt;% unnest(test, test_direction) %&gt;% mutate(correct = if_else(test_direction == direction, 1, 0)) %&gt;% summarise(test_accuracy = mean(correct), test_error = 1 - test_accuracy) ## # A tibble: 1 x 2 ## test_accuracy test_error ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.480 0.520 # Confusion matrix for test dataset demo_tib %&gt;% unnest(test, test_direction) %&gt;% mutate(correct = if_else(test_direction == direction, 1, 0)) %&gt;% count(test_direction, direction) %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 4 x 4 ## test_direction direction n prop ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Down Down 77 0.306 ## 2 Down Up 97 0.385 ## 3 Up Down 34 0.135 ## 4 Up Up 44 0.175 Time to add some other models. glm_fits &lt;- data_db %&gt;% mutate(mod_01 = map(train, glm, formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + volume, family = binomial), mod_02 = map(train, glm, formula = direction ~ lag1 + lag2, family = binomial), mod_03 = map(train, glm, formula = direction ~ lag1*lag2, family = binomial), mod_04 = map(train, glm, formula = direction ~ lag1 + volume, family = binomial), mod_05 = map(train, glm, formula = direction ~ lag1*volume, family = binomial)) %&gt;% gather(key = model_name, value = model_fit, contains(&quot;mod_&quot;)) # Function to calculate error rate error_rate_glm &lt;- function(data, model){ data %&gt;% mutate(pred_prob = predict(model, newdata = data, type = &quot;response&quot;), pred_direction = if_else(pred_prob &gt; 0.5, &quot;Up&quot;, &quot;Down&quot;), error = pred_direction != direction) %&gt;% pull(error) %&gt;% mean() } # Function to form confusion matrix confusion_mat_glm &lt;- function(data, model){ data %&gt;% mutate(pred_prob = predict(model, newdata = data, type = &quot;response&quot;), pred_direction = if_else(pred_prob &gt; 0.5, &quot;Up&quot;, &quot;Down&quot;)) %&gt;% count(direction, pred_direction) %&gt;% mutate(prop = n / sum(n)) } # Calculate model error glm_fits &lt;- glm_fits %&gt;% mutate(train_error = map2_dbl(train, model_fit, error_rate_glm), test_error = map2_dbl(test, model_fit, error_rate_glm), test_confusion = map2(test, model_fit, confusion_mat_glm)) glm_fits %&gt;% select(model_name, train_error, test_error) %&gt;% # select_if(~ !is_list(.)) %&gt;% arrange(test_error) %&gt;% kable() model_name train_error test_error mod_02 0.4839679 0.4404762 mod_03 0.4829659 0.4404762 mod_04 0.4709419 0.4841270 mod_05 0.4619238 0.5039683 mod_01 0.4729459 0.5198413 glm_fits %&gt;% filter(model_name == &quot;mod_02&quot;) %&gt;% unnest(test_confusion) ## # A tibble: 4 x 7 ## model_name train_error test_error direction pred_direction n prop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 mod_02 0.484 0.440 Down Down 35 0.139 ## 2 mod_02 0.484 0.440 Down Up 76 0.302 ## 3 mod_02 0.484 0.440 Up Down 35 0.139 ## 4 mod_02 0.484 0.440 Up Up 106 0.421 4.3 Linear Discriminant Analysis # Fit lda models lda_fits &lt;- data_db %&gt;% mutate(mod_01 = map(train, ~ lda(formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + volume, data = .x)), mod_02 = map(train, ~ lda(formula = direction ~ lag1 + lag2, data = .x)), mod_03 = map(train, ~ lda(formula = direction ~ lag1*lag2, data = .x)), mod_04 = map(train, ~ lda(formula = direction ~ lag1 + volume, data = .x)), mod_05 = map(train, ~ lda(formula = direction ~ lag1*volume, data = .x))) %&gt;% gather(key = model_name, value = model_fit, contains(&quot;mod_&quot;)) # Function to calculate lda error rate error_rate_lda &lt;- function(data, model){ data %&gt;% mutate(pred_direction = predict(model, newdata = data) %&gt;% pluck(&quot;class&quot;), error = pred_direction != direction) %&gt;% pull(error) %&gt;% mean() } # Function to form lda confusion matrix confusion_mat_lda &lt;- function(data, model){ data %&gt;% mutate(pred_direction = predict(model, newdata = data) %&gt;% pluck(&quot;class&quot;)) %&gt;% count(direction, pred_direction) %&gt;% mutate(prop = n / sum(n)) } # update lda_fits with error and confusion info lda_fits &lt;- lda_fits %&gt;% mutate(train_error = map2_dbl(train, model_fit, error_rate_lda), test_error = map2_dbl(test, model_fit, error_rate_lda), test_confusion = map2(test, model_fit, confusion_mat_lda)) # Compare models by test_error lda_fits %&gt;% select(model_name, train_error, test_error) %&gt;% # select_if(~ !is_list(.)) %&gt;% arrange(test_error) %&gt;% kable() model_name train_error test_error mod_02 0.4839679 0.4404762 mod_03 0.4839679 0.4404762 mod_04 0.4709419 0.4841270 mod_05 0.4619238 0.5079365 mod_01 0.4719439 0.5198413 # Get confusion matrix for best model lda_fits %&gt;% filter(model_name == &quot;mod_02&quot;) %&gt;% unnest(test_confusion) ## # A tibble: 4 x 7 ## model_name train_error test_error direction pred_direction n prop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 mod_02 0.484 0.440 Down Down 35 0.139 ## 2 mod_02 0.484 0.440 Down Up 76 0.302 ## 3 mod_02 0.484 0.440 Up Down 35 0.139 ## 4 mod_02 0.484 0.440 Up Up 106 0.421 4.4 Quadratic Discriminant Analysis # Fit qda models qda_fits &lt;- data_db %&gt;% mutate(mod_01 = map(train, ~ qda(formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + volume, data = .x)), mod_02 = map(train, ~ qda(formula = direction ~ lag1 + lag2, data = .x)), mod_03 = map(train, ~ qda(formula = direction ~ lag1*lag2, data = .x)), mod_04 = map(train, ~ qda(formula = direction ~ lag1 + volume, data = .x)), mod_05 = map(train, ~ qda(formula = direction ~ lag1*volume, data = .x))) %&gt;% gather(key = model_name, value = model_fit, contains(&quot;mod_&quot;)) # Function to calculate qda error rate error_rate_qda &lt;- function(data, model){ data %&gt;% mutate(pred_direction = predict(model, newdata = data) %&gt;% pluck(&quot;class&quot;), error = pred_direction != direction) %&gt;% pull(error) %&gt;% mean() } # Function to form qda confusion matrix confusion_mat_qda &lt;- function(data, model){ data %&gt;% mutate(pred_direction = predict(model, newdata = data) %&gt;% pluck(&quot;class&quot;)) %&gt;% count(direction, pred_direction) %&gt;% mutate(prop = n / sum(n)) } # update qda_fits with error and confusion info qda_fits &lt;- qda_fits %&gt;% mutate(train_error = map2_dbl(train, model_fit, error_rate_qda), test_error = map2_dbl(test, model_fit, error_rate_qda), test_confusion = map2(test, model_fit, confusion_mat_qda)) # Compare models by test_error qda_fits %&gt;% select(model_name, train_error, test_error) %&gt;% # select_if(~ !is_list(.)) %&gt;% arrange(test_error) %&gt;% kable() model_name train_error test_error mod_02 0.4859719 0.4007937 mod_03 0.4669339 0.4603175 mod_05 0.4969940 0.4722222 mod_04 0.5040080 0.5436508 mod_01 0.4579158 0.5555556 # Get confusion matrix for best model qda_fits %&gt;% filter(model_name == &quot;mod_02&quot;) %&gt;% unnest(test_confusion) ## # A tibble: 4 x 7 ## model_name train_error test_error direction pred_direction n prop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 mod_02 0.486 0.401 Down Down 30 0.119 ## 2 mod_02 0.486 0.401 Down Up 81 0.321 ## 3 mod_02 0.486 0.401 Up Down 20 0.0794 ## 4 mod_02 0.486 0.401 Up Up 121 0.480 4.5 K-Nearest Neighbors #### Helper Functions # tidy wrapper function for knn knn_tidy &lt;- function(train, test, pred_vars, response_var, ...){ train_reduced &lt;- train %&gt;% select(!!pred_vars) %&gt;% as.matrix() test_reduced &lt;- test %&gt;% select(!!pred_vars) %&gt;% as.matrix() train_class &lt;- train %&gt;% select(!!response_var) %&gt;% as.matrix() preds &lt;- class::knn(train = train_reduced, test = test_reduced, cl = train_class, ...) pred_name &lt;- paste0(&quot;pred_&quot;, response_var) tibble(!!pred_name := preds) } # Function to calculate knn error rate error_rate_knn &lt;- function(data, pred_value){ data %&gt;% bind_cols(pred_value) %&gt;% mutate(error = direction != pred_direction) %&gt;% pull(error) %&gt;% mean() } # Function to form knn confusion matrix confusion_mat_knn &lt;- function(data, pred_value){ data %&gt;% bind_cols(pred_value) %&gt;% count(direction, pred_direction) %&gt;% mutate(prop = n / sum(n)) } # Set-up tibble with predictor vars pred_var &lt;- tibble(pred_set = list(c(&quot;lag1&quot;, &quot;lag2&quot;, &quot;lag3&quot;, &quot;lag4&quot;, &quot;lag5&quot;, &quot;volume&quot;), c(&quot;lag1&quot;, &quot;lag2&quot;))) # Set-up tibble with num of neighbors (k) k_values &lt;- tibble(k_value = c(1, 5, 10, 15, 20, 40)) # Set-up tibble with model fitting info &amp; fit to test dataset knn_fits &lt;- data_db %&gt;% crossing(k_values) %&gt;% crossing(pred_var) %&gt;% mutate(knn_preds = pmap(list(train, test, pred_set,&quot;direction&quot;, k_value), knn_tidy)) # update knn_fits with error and confusion info knn_fits &lt;- knn_fits %&gt;% mutate(test_error = map2_dbl(test, knn_preds, error_rate_knn), test_confusion = map2(test, knn_preds, confusion_mat_knn)) # Compare models by test_error knn_fits %&gt;% select(pred_set, k_value, test_error) %&gt;% arrange(test_error) %&gt;% kable() pred_set k_value test_error c(“lag1”, “lag2”, “lag3”, “lag4”, “lag5”, “volume”) 1 0.4880952 c(“lag1”, “lag2”, “lag3”, “lag4”, “lag5”, “volume”) 20 0.4880952 c(“lag1”, “lag2”) 20 0.4880952 c(“lag1”, “lag2”, “lag3”, “lag4”, “lag5”, “volume”) 40 0.4880952 c(“lag1”, “lag2”) 1 0.5000000 c(“lag1”, “lag2”, “lag3”, “lag4”, “lag5”, “volume”) 5 0.5000000 c(“lag1”, “lag2”) 10 0.5079365 c(“lag1”, “lag2”, “lag3”, “lag4”, “lag5”, “volume”) 10 0.5119048 c(“lag1”, “lag2”, “lag3”, “lag4”, “lag5”, “volume”) 15 0.5119048 c(“lag1”, “lag2”) 40 0.5119048 c(“lag1”, “lag2”) 5 0.5158730 c(“lag1”, “lag2”) 15 0.5158730 # Get confusion matrix for best model knn_fits %&gt;% arrange(test_error) %&gt;% pluck(&quot;test_confusion&quot;, 1) ## # A tibble: 4 x 4 ## direction pred_direction n prop ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Down Down 50 0.198 ## 2 Down Up 61 0.242 ## 3 Up Down 62 0.246 ## 4 Up Up 79 0.313 "],
["lab-cross-validation-and-the-bootstrap.html", "5 Lab: Cross-Validation and the Bootstrap 5.1 Validation Set Approach 5.2 Leave-One-Out-Cross Validation 5.3 \\(k\\)-fold Cross-Validation 5.4 The Bootstrap", " 5 Lab: Cross-Validation and the Bootstrap This is a modified version of the Lab: Cross-Validation and the Bootstrap section of chapter 5 from Introduction to Statistical Learning with Application in R. This version uses tidyverse techniques and methods that will allow for scalability and a more efficient data analytic pipeline. We will need the packages loaded below. # Load packages library(tidyverse) library(modelr) library(janitor) library(skimr) Whenever performing analyses or processes that include randomization or resampling it is considered best practice to set the seed of your software’s random number generator. This is done in R using set.seed(). This ensure the analyses and procedures being performed are reproducible. For instance, readers following along in the lab will be able to produce the precisely the same results as those produced in the lab. Provided they run all code in the lab in sequence from start to finish — cannot run code chucks out of order. Setting the seed should occur towards the top of an analytic script, say directly after loading necessary packages. # Set seed set.seed(27182) # Used digits from e 5.1 Validation Set Approach We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto dataset. This dataset is from the ISLR library. Take a moment and inspect the codebook — ?ISLR::Auto. We will read in the data from the Auto.csv file and process do a little processing. auto_dat &lt;- read_csv(&quot;data/Auto.csv&quot;) %&gt;% clean_names() We begin by using the sample() function to split the set of observations sample() into two halves, by selecting a random subset of 196 observations out of the original 392 observations. We refer to these observations as the training set. auto_validation &lt;- tibble(train = auto_dat %&gt;% sample_n(196) %&gt;% list(), test = auto_dat %&gt;% setdiff(train) %&gt;% list()) Let’s keep it relatively simple and fit a simple linear regression using horsepower to predict mpg and polynomial regressions of up to degree 5 of horsepower to predict mpg. # Setup tibble with model names and formulas model_def &lt;- tibble(degree = 1:10, fmla = str_c(&quot;mpg ~ poly(horsepower, &quot;, degree, &quot;)&quot;)) # Combine validation setup with model fitting info auto_validation &lt;- auto_validation %&gt;% crossing(model_def) # Add model fits and assessment auto_validation &lt;- auto_validation %&gt;% mutate(model_fit = map2(fmla, train, lm), test_mse = map2_dbl(model_fit, test, mse)) auto_validation %&gt;% select(degree, test_mse) %&gt;% # arrange(test_mse) %&gt;% kable() degree test_mse 1 26.04603 2 19.98592 3 19.98956 4 20.21181 5 20.46329 6 20.91796 7 20.76588 8 20.78421 9 20.89817 10 20.81535 auto_validation %&gt;% select(degree, test_mse) %&gt;% ggplot(aes(x = degree, y = test_mse)) + geom_line() 5.2 Leave-One-Out-Cross Validation auto_loocv &lt;- auto_dat %&gt;% crossv_kfold(nrow(auto_dat), id = &quot;fold&quot;) auto_loocv &lt;- auto_loocv %&gt;% crossing(model_def) %&gt;% mutate(model_fit = map2(fmla, train, lm), fold_mse = map2_dbl(model_fit, test, mse)) auto_loocv %&gt;% group_by(degree) %&gt;% summarise(test_mse = mean(fold_mse)) ## # A tibble: 10 x 2 ## degree test_mse ## &lt;int&gt; &lt;dbl&gt; ## 1 1 24.2 ## 2 2 19.2 ## 3 3 19.3 ## 4 4 19.4 ## 5 5 19.0 ## 6 6 19.0 ## 7 7 18.8 ## 8 8 19.0 ## 9 9 19.1 ## 10 10 19.5 auto_loocv %&gt;% group_by(degree) %&gt;% summarise(test_mse = mean(fold_mse)) %&gt;% ggplot(aes(x = degree, y = test_mse)) + geom_line() 5.3 \\(k\\)-fold Cross-Validation auto_10fold &lt;- auto_dat %&gt;% crossv_kfold(10, id = &quot;fold&quot;) auto_10fold &lt;- auto_10fold %&gt;% crossing(model_def) %&gt;% mutate(model_fit = map2(fmla, train, lm), fold_mse = map2_dbl(model_fit, test, mse)) auto_10fold %&gt;% ggplot(aes(x = degree, y = fold_mse, color = fold)) + geom_line() auto_10fold %&gt;% group_by(degree) %&gt;% summarize(test_mse = mean(fold_mse)) %&gt;% ggplot(aes(x = degree, y = test_mse)) + geom_line() + geom_point() 5.4 The Bootstrap We will we using the Portfolio dataset from ISLR — see ?ISLR::Portfolio for details. We will load the dataset from the Portfolio.csv file. portfolio_dat &lt;- read_csv(&quot;data/Portfolio.csv&quot;) %&gt;% clean_names() portfolio_dat %&gt;% skim() ## Skim summary statistics ## n obs: 100 ## n variables: 2 ## ## -- Variable type:numeric --------------------------------------------------------------------------- ## variable missing complete n mean sd p0 p25 p50 p75 p100 ## x 0 100 100 -0.077 1.06 -2.43 -0.89 -0.27 0.56 2.46 ## y 0 100 100 -0.097 1.14 -2.73 -0.89 -0.23 0.81 2.57 ## hist ## &lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2582&gt; ## &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2583&gt;&lt;U+2581&gt; 5.4.1 Estimating the Accuracy of a Statistic of Interest # Statistic of interest alpha_fn &lt;- function(resample_obj){ resample_obj %&gt;% # turn resample object into dataset as_tibble() %&gt;% summarise(alpha = (var(y)-cov(x,y))/(var(x)+var(y)-2*cov(x,y))) %&gt;% pull(alpha) } # Estimate on original data original_est &lt;- portfolio_dat %&gt;% alpha_fn() original_est ## [1] 0.5758321 # create 1000 bootstrap estimates portfolio_boot &lt;- portfolio_dat %&gt;% bootstrap(1000, id = &quot;boot_id&quot;) %&gt;% mutate(alpha_boot = map_dbl(strap, alpha_fn)) # Summary of bootstrap estimates portfolio_boot %&gt;% select(alpha_boot) %&gt;% skim() ## Skim summary statistics ## n obs: 1000 ## n variables: 1 ## ## -- Variable type:numeric --------------------------------------------------------------------------- ## variable missing complete n mean sd p0 p25 p50 p75 p100 ## alpha_boot 0 1000 1000 0.58 0.092 0.29 0.51 0.58 0.64 0.87 ## hist ## &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;&lt;U+2581&gt; # Boxplot of bootstrap estimates with estimate from original data (red X) portfolio_boot %&gt;% ggplot(aes(x = &quot;alpha_boot&quot; , y = alpha_boot)) + geom_boxplot() + geom_point(aes(x = &quot;alpha_boot&quot;, y = original_est), shape = 4, size = 4, color = &quot;red&quot;) + coord_flip() # Histogram of bootstrap estimates with estimate from original data (red dashed line) portfolio_boot %&gt;% ggplot(aes(x = alpha_boot)) + geom_histogram(bins = 25) + geom_vline(aes(xintercept = original_est), color = &quot;red&quot;, linetype = &quot;dashed&quot;) # Bootstrap estimate of standard error for estimator portfolio_boot %&gt;% summarise(est_boot = mean(alpha_boot), est_se = sd(alpha_boot)) ## # A tibble: 1 x 2 ## est_boot est_se ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.578 0.0921 5.4.2 Estimating the Accuracy of a Linear Regression Model # 1000 Bootstraps, fit models, get parameter estimates auto_boot &lt;- auto_dat %&gt;% bootstrap(1000, id = &quot;boot_id&quot;) %&gt;% mutate(model_fit = map(strap, lm, formula = mpg ~ horsepower), mod_tidy = map(model_fit, broom::tidy)) # Examine bootstrap coefficient estimates auto_boot %&gt;% unnest(mod_tidy) %&gt;% group_by(term) %&gt;% select(term, estimate) %&gt;% skim() %&gt;% kable() ## Skim summary statistics ## n obs: 2000 ## n variables: 2 ## ## Variable type: numeric ## ## term variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## ------------- ---------- --------- ---------- ------ ------- -------- ------- ------- ------- ------- ------- ---------- ## (Intercept) estimate 0 1000 1000 39.95 0.89 37.47 39.38 39.98 40.5 42.91 &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## horsepower estimate 0 1000 1000 -0.16 0.0077 -0.18 -0.16 -0.16 -0.15 -0.14 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2582&gt;&lt;U+2581&gt; # Histogram of bootstrap coefficient estimates auto_boot %&gt;% unnest(mod_tidy) %&gt;% ggplot(aes(x = estimate)) + geom_histogram() + facet_wrap(. ~ term, scale = &quot;free_x&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Boxplot of bootstrap coefficient estimates auto_boot %&gt;% unnest(mod_tidy) %&gt;% ggplot(aes(x = &quot;&quot;, y = estimate)) + geom_boxplot() + facet_wrap(. ~ term, scale = &quot;free_x&quot;) + coord_flip() # Estimates using original data (including SE) auto_dat %&gt;% lm(mpg ~ horsepower, data = .) %&gt;% broom::tidy() %&gt;% select(-statistic, -p.value) ## # A tibble: 2 x 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 39.9 0.717 ## 2 horsepower -0.158 0.00645 # Estimates using bootstrap auto_boot %&gt;% unnest(mod_tidy) %&gt;% group_by(term) %&gt;% summarise(boot_est = mean(estimate), est_se = sd(estimate)) ## # A tibble: 2 x 3 ## term boot_est est_se ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 40.0 0.888 ## 2 horsepower -0.158 0.00770 "],
["glossary-of-terms.html", "6 Glossary of Terms", " 6 Glossary of Terms "],
["helpful-references.html", "7 Helpful References", " 7 Helpful References "]
]
